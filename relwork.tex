\section{Related Work}
\label{sec:relwork}
\label{sec:relatedwork}

\minihead{Dimensionality Reduction} DR is a
classic operation~\cite{dr-survey1,dr-survey2,trefethen,nonlinear-dr} that is
well studied in the
database~\cite{keogh-indexing,local-dr,charu-ss,dynamic-ss}, data
mining~\cite{sax,paa}, statistics and machine
learning~\cite{alecton,shamir}, and theoretical CS~\cite{bernstein,pca-stoc} communities.

Recent breakthroughs in the theoretical
statistics community provided new algorithms for PCA that promise
substantial scalability improvements without compromising result
quality~\cite{alecton,tropp,re-new}. 
Foremost among these techniques are advanced stochastic methods~\cite{re-new,shamir}, and techniques for randomized SVD~\cite{tropp}.
While we default to the latter for use by DROP's PCA operator, DROP's modular architecture makes it simple to use any method in its place, including recent systems advances in scalable PCA~\cite{ppca-sigmod}.
%As a proof of concept of our method, we provide implementations of full SVD-based PCA, power iteration, as well as Oja's method. 
To the best of our knowledge, advanced methods for PCA
have not been empirically compared head-to-head with conventional
dimensionality reduction approaches such as Piecewise Approximate
Averaging~\cite{paa}, especially on real datasets. 
In addition, DROP
\emph{combines} these methods with row-level sampling to provide benefits similar to using stochastic methods for PCA, regardless of the chosen PCA subroutine. 

%This setting differs from that of Moving Window (or Rolling) PCA in that the these methods assume overlap among the data samples, whereas here our samples are independently drawn from the same underlying data distribution~\cite{mwpca}.

%\red{
%\minihead{Time Series Indexing}
%While DROP is intended as a general purpose DR operator for downstream workloads, there exists a vast body of literature specific to time series indexing for similarity search. 
%While these techniques, such as iSAX2+ (and related methods)~\cite{sax,isax,isaxorig,hotsax}, SSH~\cite{ssh}, and Coconut~\cite{coconut} are highly optimized for the bulk-load and repeated query use case, DROP provides a more flexible, downstream-operator aware method. 
%}

\minihead{Approximate Query Processing} 
Inspired by approximate query processing engines~\cite{barzan-keynote}
as in online aggregation~\cite{onlineagg}, DROP performs progressive
sampling.  In contrast
with more general data dimensionality estimation
methods~\cite{dr-estimation}, DROP optimizes for
$TLB$. As we illustrated in Section~\ref{sec:experiments}, this
strategy confers substantial runtime improvements.
While DROP performs simple uniform sampling, the literature contains a wealth of techniques for various biased sampling techniques~\cite{surajit-sample, surajit-2}.
Finally, DROP performs online progress estimation to minimize the
end-to-end analytics cost function. This is analogous to query
progress estimation~\cite{qpi1} and performance
prediction~\cite{mr-predict} in database and data
warehouse settings and has been exploited in approximate query
processing engines such as BlinkDB~\cite{blinkdb}. 

\minihead{Scalable \red{ Workload-Aware, }Complex Analytics} DROP is an operator
for analytics dataflow pipelines. Thus, DROP is
as an extension of recent results on integrating complex
analytics function including model training~\cite{bismarck,mcdb,mlbase} and
data exploration~\cite{scorpion,canopy,kraska-viz} operators into analytics engines. 
%\red{In particular, DROP is especially related to recent work in integrating workload-aware cost models to complex subscription forecasting models~\cite{forecasting} so as to reduce subscriber notification overhead.}
